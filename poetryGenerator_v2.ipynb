{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "poetryGenerator_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7c315517838442eea5cd262322d45278": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e09f7fbaf8b640c795a399f11e351d25",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f2bb7ec41a8c47d69ec6c540864a36af",
              "IPY_MODEL_a0c556df208c4dfc90b8597e8cbec2fa"
            ]
          }
        },
        "e09f7fbaf8b640c795a399f11e351d25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f2bb7ec41a8c47d69ec6c540864a36af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_67b9a5cd58c8455a89c9a981c4aee042",
            "_dom_classes": [],
            "description": "training routine: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 100,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 100,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_99bd9c81451b41699dd6ca623e82829e"
          }
        },
        "a0c556df208c4dfc90b8597e8cbec2fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ddec5362159d40da850556e39c441af2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 100/100 [08:51&lt;00:00,  5.29s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0b5ffecce8f2478394f8639191a659f4"
          }
        },
        "67b9a5cd58c8455a89c9a981c4aee042": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "99bd9c81451b41699dd6ca623e82829e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ddec5362159d40da850556e39c441af2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0b5ffecce8f2478394f8639191a659f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4d665657121d4044ae287f1fda164d51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fd21a161ba354202a2097b1690ec2867",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d9d365346f1645fbb10ef18779b82dd0",
              "IPY_MODEL_14780bfe300841d6882ace9e12a3c725"
            ]
          }
        },
        "fd21a161ba354202a2097b1690ec2867": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d9d365346f1645fbb10ef18779b82dd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9538779411644b4da1774a4557b92d90",
            "_dom_classes": [],
            "description": "split=train:  93%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 14,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 13,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f2b0c6f19d754fb5970ac621785c54db"
          }
        },
        "14780bfe300841d6882ace9e12a3c725": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c70069eb7fbc4061b8e5570ca176a163",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 13/14 [08:51&lt;00:00,  2.93it/s, acc=85.1, epoch=99, loss=0.77]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_158f92552a43458c9b9ae46170c47eda"
          }
        },
        "9538779411644b4da1774a4557b92d90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f2b0c6f19d754fb5970ac621785c54db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c70069eb7fbc4061b8e5570ca176a163": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "158f92552a43458c9b9ae46170c47eda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "45f267023c55457a9e08dc54d4e5721b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_55d09f1310e34394b7c76c77074b7c91",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_eee5a78fcafc448da9eaf3456c1e9648",
              "IPY_MODEL_138a29d029b947aebdfb1e3510fe51d8"
            ]
          }
        },
        "55d09f1310e34394b7c76c77074b7c91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "eee5a78fcafc448da9eaf3456c1e9648": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_146bcee8e92544048999e086cb62dfad",
            "_dom_classes": [],
            "description": "split=val:  67%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 3,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_22f2f3ffad6949959011617388ebc0eb"
          }
        },
        "138a29d029b947aebdfb1e3510fe51d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5be9a88ded444eef8bf4e5528e092881",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2/3 [08:51&lt;00:02,  2.67s/it, acc=27.5, epoch=99, loss=7.8]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7f564389953148fc95b9707870a35ebf"
          }
        },
        "146bcee8e92544048999e086cb62dfad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "22f2f3ffad6949959011617388ebc0eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5be9a88ded444eef8bf4e5528e092881": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7f564389953148fc95b9707870a35ebf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfdW2mKNtN12"
      },
      "source": [
        "# Sequence Modelling using bidirectional GRU and Attention Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "110eHlq7xKEf"
      },
      "source": [
        "## References\n",
        "\n",
        "__DATA__\n",
        "\n",
        "1. https://www.kaggle.com/ultrajack/modern-renaissance-poetry/data\n",
        "\n",
        "__Code__\n",
        "\n",
        "1. https://github.com/joosthub/PyTorchNLPBook/blob/master/chapters/chapter_5/5_2_CBOW/5_2_munging_frankenstein.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd7AnCfPpYo4"
      },
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "import string\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from argparse import Namespace\n",
        "import torch.optim as optim\n",
        "from tqdm.notebook import tqdm\n",
        "import torch.nn.functional as F\n",
        "import re\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGcKzMc8sOWp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a21ef800-3794-4167-a11f-82eb7ab7811f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Wm4HQCC2-I7"
      },
      "source": [
        "## Dataset Build up\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu9RXacc3D6q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "37ec1012-397d-4f89-8776-755332a64605"
      },
      "source": [
        "poet_df = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/PoetryGenerator/data/poetryfoundation.csv\")\n",
        "poet_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>content</th>\n",
              "      <th>poem name</th>\n",
              "      <th>age</th>\n",
              "      <th>type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>WILLIAM SHAKESPEARE</td>\n",
              "      <td>Let the bird of loudest lay\\r\\nOn the sole Ara...</td>\n",
              "      <td>The Phoenix and the Turtle</td>\n",
              "      <td>Renaissance</td>\n",
              "      <td>Mythology &amp; Folklore</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>DUCHESS OF NEWCASTLE MARGARET CAVENDISH</td>\n",
              "      <td>Sir Charles into my chamber coming in,\\r\\nWhen...</td>\n",
              "      <td>An Epilogue to the Above</td>\n",
              "      <td>Renaissance</td>\n",
              "      <td>Mythology &amp; Folklore</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>THOMAS BASTARD</td>\n",
              "      <td>Our vice runs beyond all that old men saw,\\r\\n...</td>\n",
              "      <td>Book 7, Epigram 42</td>\n",
              "      <td>Renaissance</td>\n",
              "      <td>Mythology &amp; Folklore</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>EDMUND SPENSER</td>\n",
              "      <td>Lo I the man, whose Muse whilome did maske,\\r\\...</td>\n",
              "      <td>from The Faerie Queene: Book I, Canto I</td>\n",
              "      <td>Renaissance</td>\n",
              "      <td>Mythology &amp; Folklore</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RICHARD BARNFIELD</td>\n",
              "      <td>Long have I longd to see my love againe,\\r\\nSt...</td>\n",
              "      <td>Sonnet 16</td>\n",
              "      <td>Renaissance</td>\n",
              "      <td>Mythology &amp; Folklore</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    author  ...                  type\n",
              "0                      WILLIAM SHAKESPEARE  ...  Mythology & Folklore\n",
              "1  DUCHESS OF NEWCASTLE MARGARET CAVENDISH  ...  Mythology & Folklore\n",
              "2                           THOMAS BASTARD  ...  Mythology & Folklore\n",
              "3                           EDMUND SPENSER  ...  Mythology & Folklore\n",
              "4                        RICHARD BARNFIELD  ...  Mythology & Folklore\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVbZy_YD4LCj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6794898-84c4-44a1-a37b-c9fbee713fff"
      },
      "source": [
        "poet_df['author'].unique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['WILLIAM SHAKESPEARE', 'DUCHESS OF NEWCASTLE MARGARET CAVENDISH',\n",
              "       'THOMAS BASTARD', 'EDMUND SPENSER', 'RICHARD BARNFIELD',\n",
              "       'SIR WALTER RALEGH', 'QUEEN ELIZABETH I', 'JOHN DONNE',\n",
              "       'JOHN SKELTON', 'CHRISTOPHER MARLOWE', 'LADY MARY WROTH',\n",
              "       'ROBERT SOUTHWELL, SJ', 'WILLIAM BYRD', 'GEORGE GASCOIGNE',\n",
              "       'HENRY VIII, KING OF ENGLAND', 'SIR THOMAS WYATT', 'EN JONSON',\n",
              "       'ORLANDO GIBBONS', 'THOMAS NASHE', 'SIR PHILIP SIDNEY',\n",
              "       'SECOND BARON VAUX OF HARROWDEN THOMAS, LORD VAUX',\n",
              "       'HENRY HOWARD, EARL OF SURREY', 'GEORGE CHAPMAN', 'THOMAS CAMPION',\n",
              "       'ISABELLA WHITNEY', 'SAMUEL DANIEL', 'THOMAS HEYWOOD',\n",
              "       'GIOVANNI BATTISTA GUARINI', 'SIR EDWARD DYER', 'THOMAS LODGE',\n",
              "       'JOHN FLETCHER', 'EDGAR LEE MASTERS', 'WILLIAM BUTLER YEATS',\n",
              "       'FORD MADOX FORD', 'IVOR GURNEY', 'CARL SANDBURG', 'EZRA POUND',\n",
              "       'ELINOR WYLIE', 'GEORGE SANTAYANA', 'LOUISE BOGAN',\n",
              "       'KENNETH SLESSOR', 'HART CRANE', 'D. H. LAWRENCE',\n",
              "       'HUGH MACDIARMID', 'E. E. CUMMINGS', 'LOUIS UNTERMEYER',\n",
              "       'WALLACE STEVENS', 'MARJORIE PICKTHALL', 'RICHARD ALDINGTON',\n",
              "       'GUILLAUME APOLLINAIRE', 'SAMUEL GREENBERG', 'STEPHEN SPENDER',\n",
              "       'EDITH SITWELL', 'PAUL LAURENCE DUNBAR', 'SARA TEASDALE',\n",
              "       'MINA LOY', 'MARIANNE MOORE', 'ASIL BUNTING', 'MICHAEL ANANIA',\n",
              "       'ARCHIBALD MACLEISH', 'CONRAD AIKEN', 'MALCOLM COWLEY',\n",
              "       'KATHERINE MANSFIELD', 'T. S. ELIOT', 'GERTRUDE STEIN',\n",
              "       'JAMES JOYCE', 'KENNETH FEARING'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQB8es0I3ogt"
      },
      "source": [
        "### import NLTK and Tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3fyzhdn3udR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79c1cf33-9c10-4f1f-b71b-5405940aab0e"
      },
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Cp8wE8X3bx9"
      },
      "source": [
        "import nltk.data\n",
        "\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "poet_dataset = {'first': [], 'second': []}\n",
        "\n",
        "num_poets = 500\n",
        "\n",
        "for i in range(num_poets):\n",
        "    sentences = tokenizer.tokenize(poet_df['content'][i])\n",
        "    sentences_len = len(sentences)\n",
        "    WINDOW_SIZE = 2\n",
        "    for index in range(sentences_len - WINDOW_SIZE):\n",
        "        prev_sentence = sentences[index]\n",
        "        next_sentence = sentences[index + 1]\n",
        "\n",
        "        # print(\"1 \" + prev_sentence)\n",
        "        # print(\"2 \" + next_sentence)\n",
        "        poet_dataset['first'].append(prev_sentence)\n",
        "        poet_dataset['second'].append(next_sentence)\n",
        "\n",
        "poet_dataset = pd.DataFrame.from_dict(poet_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhUagv32CuYn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "4fc1bb30-ac7e-470d-d639-e0d6adc668f7"
      },
      "source": [
        "poet_dataset.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>first</th>\n",
              "      <th>second</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Let the bird of loudest lay\\r\\nOn the sole Ara...</td>\n",
              "      <td>But thou shrieking harbinger,\\r\\nFoul precurre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>But thou shrieking harbinger,\\r\\nFoul precurre...</td>\n",
              "      <td>From this session interdict\\r\\nEvery fowl of t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>From this session interdict\\r\\nEvery fowl of t...</td>\n",
              "      <td>Let the priest in surplice white,\\r\\nThat defu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Let the priest in surplice white,\\r\\nThat defu...</td>\n",
              "      <td>And thou treble-dated crow,\\r\\nThat thy sable ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>And thou treble-dated crow,\\r\\nThat thy sable ...</td>\n",
              "      <td>Here the anthem doth commence:\\r\\nLove and con...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               first                                             second\n",
              "0  Let the bird of loudest lay\\r\\nOn the sole Ara...  But thou shrieking harbinger,\\r\\nFoul precurre...\n",
              "1  But thou shrieking harbinger,\\r\\nFoul precurre...  From this session interdict\\r\\nEvery fowl of t...\n",
              "2  From this session interdict\\r\\nEvery fowl of t...  Let the priest in surplice white,\\r\\nThat defu...\n",
              "3  Let the priest in surplice white,\\r\\nThat defu...  And thou treble-dated crow,\\r\\nThat thy sable ...\n",
              "4  And thou treble-dated crow,\\r\\nThat thy sable ...  Here the anthem doth commence:\\r\\nLove and con..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1I_LSeu31F1"
      },
      "source": [
        "### Document Cleansing and Formating Train Data\n",
        "\n",
        "the train data would be\n",
        "\n",
        "prev_sentence | next_sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZVY2A0h34Pg"
      },
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = ' '.join(word.lower() for word in text.split(\" \"))\n",
        "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
        "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
        "    return text\n",
        "\n",
        "def preprocess_row(row):\n",
        "    row['first'] = preprocess_text(row['first'])\n",
        "    row['second'] = preprocess_text(row['second'])\n",
        "    return row"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYSJhbQw36Uv"
      },
      "source": [
        "poet_dataset = poet_dataset.apply(lambda row: preprocess_row(row), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bT8YWBkVDbsm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "b6380b43-d661-4bbc-e73a-4a94164ba120"
      },
      "source": [
        "poet_dataset.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>first</th>\n",
              "      <th>second</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>let the bird of loudest lay on the sole arabia...</td>\n",
              "      <td>but thou shrieking harbinger , foul precurrer ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>but thou shrieking harbinger , foul precurrer ...</td>\n",
              "      <td>from this session interdict every fowl of tyra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>from this session interdict every fowl of tyra...</td>\n",
              "      <td>let the priest in surplice white , that defunc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>let the priest in surplice white , that defunc...</td>\n",
              "      <td>and thou treble dated crow , that thy sable ge...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>and thou treble dated crow , that thy sable ge...</td>\n",
              "      <td>here the anthem doth commence love and constan...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               first                                             second\n",
              "0  let the bird of loudest lay on the sole arabia...  but thou shrieking harbinger , foul precurrer ...\n",
              "1  but thou shrieking harbinger , foul precurrer ...  from this session interdict every fowl of tyra...\n",
              "2  from this session interdict every fowl of tyra...  let the priest in surplice white , that defunc...\n",
              "3  let the priest in surplice white , that defunc...  and thou treble dated crow , that thy sable ge...\n",
              "4  and thou treble dated crow , that thy sable ge...  here the anthem doth commence love and constan..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bi4VHvFNFDIW"
      },
      "source": [
        "# splitting data into\n",
        "# train val and test\n",
        "\n",
        "poet_dataset['split'] = 'train'\n",
        "def assign_label(row):\n",
        "    magic_number = np.random.randint(0, 10)\n",
        "    if magic_number > 6:\n",
        "        valortest = np.random.randint(0, 2)\n",
        "        if valortest == 0:\n",
        "            return 'val'\n",
        "        elif valortest == 1:\n",
        "            return 'test'\n",
        "    else:\n",
        "        return 'train'\n",
        "           \n",
        "poet_dataset['split'] = poet_dataset.apply(lambda row: assign_label(row['split']), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xo6VsSPbFNVU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "950fc81c-6e7b-4707-ae71-c07d8adb0468"
      },
      "source": [
        "poet_dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>first</th>\n",
              "      <th>second</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>let the bird of loudest lay on the sole arabia...</td>\n",
              "      <td>but thou shrieking harbinger , foul precurrer ...</td>\n",
              "      <td>val</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>but thou shrieking harbinger , foul precurrer ...</td>\n",
              "      <td>from this session interdict every fowl of tyra...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>from this session interdict every fowl of tyra...</td>\n",
              "      <td>let the priest in surplice white , that defunc...</td>\n",
              "      <td>val</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>let the priest in surplice white , that defunc...</td>\n",
              "      <td>and thou treble dated crow , that thy sable ge...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>and thou treble dated crow , that thy sable ge...</td>\n",
              "      <td>here the anthem doth commence love and constan...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2601</th>\n",
              "      <td>why should i blame her that she filled my days...</td>\n",
              "      <td>what could have made her peaceful with a mind ...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2602</th>\n",
              "      <td>what could have made her peaceful with a mind ...</td>\n",
              "      <td>why , what could she have done , being what sh...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2603</th>\n",
              "      <td>shy one , shy one , shy one of my heart , she ...</td>\n",
              "      <td>she carries in the dishes , and lays them in a...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2604</th>\n",
              "      <td>she carries in the dishes , and lays them in a...</td>\n",
              "      <td>to an isle in the water with her would i go .</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2605</th>\n",
              "      <td>to an isle in the water with her would i go .</td>\n",
              "      <td>she carries in the candles , and lights the cu...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2606 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  first  ...  split\n",
              "0     let the bird of loudest lay on the sole arabia...  ...    val\n",
              "1     but thou shrieking harbinger , foul precurrer ...  ...  train\n",
              "2     from this session interdict every fowl of tyra...  ...    val\n",
              "3     let the priest in surplice white , that defunc...  ...  train\n",
              "4     and thou treble dated crow , that thy sable ge...  ...  train\n",
              "...                                                 ...  ...    ...\n",
              "2601  why should i blame her that she filled my days...  ...  train\n",
              "2602  what could have made her peaceful with a mind ...  ...  train\n",
              "2603  shy one , shy one , shy one of my heart , she ...  ...  train\n",
              "2604  she carries in the dishes , and lays them in a...  ...  train\n",
              "2605     to an isle in the water with her would i go .   ...  train\n",
              "\n",
              "[2606 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxFGijGtrbPO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dd4cf0c-5110-404e-97bf-890bcb46e3e1"
      },
      "source": [
        "poet_dataset['split'].unique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['val', 'train', 'test'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJsCvpGVDwPx"
      },
      "source": [
        "poet_dataset.to_csv(\"/content/drive/My Drive/Colab Notebooks/PoetryGenerator/data/poet_dataframe_sentences.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0G7t-3MpYpL"
      },
      "source": [
        "## Sequence Vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMUE2SmwpYpN"
      },
      "source": [
        "class SequenceVocabulary(object):\n",
        "    \"\"\"Class to extract and process vocabularies for mapping\"\"\"\n",
        "    \n",
        "    def __init__(self, token_to_idx=None, mask_token=\"<MASK>\", add_unk=True, unk_token=\"<UNK>\"):\n",
        "        if token_to_idx is None:\n",
        "            token_to_idx = {}\n",
        "        self._token_to_idx = token_to_idx\n",
        "        \n",
        "        self._idx_to_token = {\n",
        "            idx: token for token, idx in self._token_to_idx.items()\n",
        "        }\n",
        "        \n",
        "        self._add_unk = add_unk\n",
        "        self._unk_token = unk_token\n",
        "        self._mask_token = mask_token\n",
        "        \n",
        "        # add begin and end sequence token\n",
        "        self._begin_of_seq_token = \"<BEGIN-OF-SEQUENCE>\"\n",
        "        self._end_of_seq_token = \"<END-OF-SEQUENCE>\"\n",
        "        \n",
        "        self.begin_seq_index = self.add_token(self._begin_of_seq_token)\n",
        "        self.end_seq_index = self.add_token(self._end_of_seq_token)\n",
        "\n",
        "        self.mask_index = self.add_token(mask_token)\n",
        "        self.unk_index = -1\n",
        "        if add_unk:\n",
        "            self.unk_index = self.add_token(unk_token)\n",
        "            \n",
        "    def to_serializeable(self):\n",
        "        \"\"\"return a serializeable dictionary\"\"\"\n",
        "        return {\n",
        "            'token_to_idx': self._token_to_idx,\n",
        "            'mask_token': self._mask_token,\n",
        "            'add_unk': self._add_unk,\n",
        "            'unk_token': self._unk_token\n",
        "        }\n",
        "    \n",
        "    @classmethod\n",
        "    def from_serializeable(cls, contents):\n",
        "        \"\"\"create vocabulary object from serialize dictionary\"\"\"\n",
        "        return cls(**contents)\n",
        "    \n",
        "    def add_token(self, token):\n",
        "        \"\"\"Add a token and return it's index\"\"\"\n",
        "        if token in self._token_to_idx:\n",
        "            index = self._token_to_idx[token]\n",
        "        else:\n",
        "            index = len(self._token_to_idx)\n",
        "            self._token_to_idx[token] = index\n",
        "            self._idx_to_token[index] = token\n",
        "        return index\n",
        "    \n",
        "    def lookup_token(self, token):\n",
        "        \"\"\"get the index of a token \n",
        "        if not exist returns the unk_index\"\"\"\n",
        "        if self.unk_index >= 0:\n",
        "            return self._token_to_idx.get(token, self.unk_index)\n",
        "        else:\n",
        "            return self._token_to_idx[token]\n",
        "        \n",
        "    def lookup_index(self, index):\n",
        "        if index not in self._idx_to_token:\n",
        "            raise KeyError(\"the index %d is not in the vocabulary\" % index)\n",
        "        return self._idx_to_token[index]\n",
        "    \n",
        "    def __str__(self):\n",
        "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self._token_to_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pj6VHBfDpYpd"
      },
      "source": [
        "## Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2AiGKcapYpf"
      },
      "source": [
        "class NMTVectorizer(object):\n",
        "    def __init__(self, source_vocab, target_vocab,\n",
        "                max_source_length, max_target_length):\n",
        "        self.source_vocab = source_vocab\n",
        "        self.target_vocab = target_vocab\n",
        "        self.max_source_length = max_source_length\n",
        "        self.max_target_length = max_target_length\n",
        "        \n",
        "    @classmethod\n",
        "    def from_dataframe(cls, lang_df):\n",
        "        source_vocab = SequenceVocabulary()\n",
        "        target_vocab = SequenceVocabulary()\n",
        "        \n",
        "        max_source_length, max_target_length = 0, 0\n",
        "        \n",
        "        for _, rows in lang_df.iterrows():\n",
        "            # source\n",
        "            source_token = rows[\"first\"].split(\" \")\n",
        "            if len(source_token) > max_source_length:\n",
        "                max_source_length = len(source_token)\n",
        "            for token in source_token:\n",
        "                source_vocab.add_token(token)\n",
        "            \n",
        "            # target\n",
        "            target_token = rows[\"second\"].split(\" \")\n",
        "            if len(target_token) > max_target_length:\n",
        "                max_target_length = len(target_token)\n",
        "            for token in target_token:\n",
        "                target_vocab.add_token(token)\n",
        "                \n",
        "        return cls(source_vocab, target_vocab,\n",
        "                  max_source_length, max_target_length)\n",
        "    \n",
        "    def _vectorize(self, indices, vector_length=-1, mask_index=0):\n",
        "        if vector_length < 0:\n",
        "            vector_length = len(indices)\n",
        "        \n",
        "        vector = np.zeros(vector_length, dtype=np.int64)\n",
        "        vector[:len(indices)] = indices\n",
        "        vector[len(indices):] = mask_index\n",
        "        return vector\n",
        "    \n",
        "    def _get_source_indices(self, source_text):\n",
        "        \"\"\"\n",
        "        Source indices adding begin_seq_index and\n",
        "        end_seq_index\n",
        "        \"\"\"\n",
        "        indices = [self.source_vocab.begin_seq_index]\n",
        "        indices.extend(self.source_vocab.lookup_token(token) for token in\n",
        "                       source_text.split(\" \"))\n",
        "        indices.append(self.source_vocab.end_seq_index)\n",
        "        \n",
        "        return indices\n",
        "    \n",
        "    def _get_target_indices(self, target_text):\n",
        "        indices = [self.target_vocab.lookup_token(token)\n",
        "                   for token in target_text.split(\" \")]\n",
        "        \n",
        "        x_indices = [self.target_vocab.begin_seq_index] + indices\n",
        "        y_indices = indices + [self.target_vocab.end_seq_index]\n",
        "        \n",
        "        return x_indices, y_indices\n",
        "    \n",
        "    def vectorize(self, source_text, target_text, use_dataset_max_length=True):\n",
        "        source_length = -1\n",
        "        target_length = -1\n",
        "        \n",
        "        if use_dataset_max_length:\n",
        "            source_length = self.max_source_length + 2\n",
        "            target_length = self.max_target_length + 1\n",
        "        \n",
        "        source_indices = self._get_source_indices(source_text)\n",
        "        source_vector = self._vectorize(source_indices,\n",
        "                                       source_length,\n",
        "                                       mask_index= self.source_vocab.mask_index)\n",
        "        \n",
        "        target_x_indices, target_y_indices = self._get_target_indices(target_text)\n",
        "        \n",
        "        target_x_vector = self._vectorize(target_x_indices,\n",
        "                                         target_length,\n",
        "                                         self.target_vocab.mask_index)\n",
        "        target_y_vector = self._vectorize(target_y_indices,\n",
        "                                         target_length,\n",
        "                                         self.target_vocab.mask_index)\n",
        "        return {\"source_vector\": source_vector,\n",
        "                \"target_x_vector\": target_x_vector,\n",
        "                \"target_y_vector\": target_y_vector,\n",
        "                \"source_length\": len(source_indices)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uc8Wgo0ApYpp"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bVL943wpYpr"
      },
      "source": [
        "class NMTDataset(Dataset):\n",
        "    def __init__(self, text_df, vectorizer):\n",
        "        self.text_df = text_df\n",
        "        self._vectorizer = vectorizer\n",
        "\n",
        "        self.train_df = self.text_df[self.text_df.split == 'train']\n",
        "        self.train_size = len(self.train_df)\n",
        "        \n",
        "        self.val_df = self.text_df[self.text_df.split == 'val']\n",
        "        self.val_size = len(self.val_df)\n",
        "        \n",
        "        self.test_df = self.text_df[self.text_df.split == 'test']\n",
        "        self.test_size = len(self.test_df)\n",
        "        \n",
        "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
        "                            'val': (self.val_df, self.val_size),\n",
        "                            'test': (self.test_df, self.test_size)}\n",
        "        \n",
        "        self.set_split('train')\n",
        "        \n",
        "    @classmethod\n",
        "    def load_dataset_and_make_vectorizer(cls, text_csv):\n",
        "        \"\"\"Load dataset from csv and returns the dataset object\n",
        "        and vectorizer\"\"\"\n",
        "        text_df = pd.read_csv(text_csv)\n",
        "        train_text_df = text_df[text_df.split == 'train']\n",
        "        return cls(text_df,\n",
        "                   NMTVectorizer.from_dataframe(train_text_df))\n",
        "    \n",
        "    def get_vectorizer(self):\n",
        "        \"\"\"Get vectorizer\"\"\"\n",
        "        return self._vectorizer\n",
        "    \n",
        "    def set_split(self, split='train'):\n",
        "        \"\"\"Set the split from data\"\"\"\n",
        "        self._target_split = split\n",
        "        self._target_df, self._target_size = self._lookup_dict[split]\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self._target_size\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"the primary entry point method for PyTorch datasets\n",
        "        Args:\n",
        "            index (int): the index to the data point\n",
        "        Returns:\n",
        "            a dict of the data point's features (x_data) and label (y_target)\n",
        "        \"\"\"\n",
        "        row = self._target_df.iloc[index]\n",
        "        \n",
        "        data_dict = self._vectorizer.vectorize(row['first'],\n",
        "                                               row['second'])\n",
        "        \n",
        "        return {\n",
        "            'x_source': data_dict[\"source_vector\"],\n",
        "            'x_target': data_dict[\"target_x_vector\"],\n",
        "            'y_target': data_dict[\"target_y_vector\"],\n",
        "            'x_source_length': data_dict[\"source_length\"]\n",
        "        }\n",
        "    \n",
        "    def get_num_batches(self, batch_size):\n",
        "        \"\"\"Given the batch size return the number of batches in the dataset\"\"\"\n",
        "        return len(self) // batch_size\n",
        "\n",
        "\n",
        "def generate_nmt_batches(dataset, batch_size, shuffle=True, drop_last=True, device=\"cpu\"):\n",
        "    \"\"\"\n",
        "    Batch Generator\n",
        "    \"\"\"\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size,\n",
        "                            shuffle=shuffle, drop_last= drop_last)\n",
        "    \n",
        "    for data_dict in dataloader:\n",
        "        lengths = data_dict['x_source_length'].numpy()\n",
        "        sorted_length_indices = lengths.argsort()[::-1].tolist()\n",
        "        \n",
        "        out_data_dict = {}\n",
        "        for name, tensor in data_dict.items():\n",
        "            out_data_dict[name] = data_dict[name][sorted_length_indices].to(device)\n",
        "        \n",
        "        yield out_data_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7hsgwKZpYp3"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7BktL1zpYp5"
      },
      "source": [
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class NMTEncoder(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_size, rnn_hidden_size):\n",
        "        \"\"\"\n",
        "        Encoder Module\n",
        "        Args:\n",
        "            num_embeddings(int): size of input dimension\n",
        "            embedding_size(int): embedding dimension\n",
        "            rnn_hidden_size(int): rnn hidden weight size\n",
        "        \"\"\"\n",
        "        super(NMTEncoder, self).__init__()\n",
        "        \n",
        "        self.source_embedding = nn.Embedding(num_embeddings, embedding_size, padding_idx=0)\n",
        "        self.birnn = nn.GRU(embedding_size, rnn_hidden_size, bidirectional=True, batch_first=True)\n",
        "        \n",
        "    def forward(self, x_source, x_lengths):\n",
        "        \"\"\"\n",
        "        Forward Pass the model\n",
        "        Args:\n",
        "            x_source (torch.Tensor): the input data tensor.\n",
        "                x_source.shape is (batch, seq_size)\n",
        "            x_lengths (torch.Tensor): a vector of lengths for each item in the batch\n",
        "        Returns:\n",
        "            a tuple: x_unpacked (torch.Tensor), x_birnn_h (torch.Tensor)\n",
        "                x_unpacked.shape = (batch, seq_size, rnn_hidden_size * 2)\n",
        "                x_birnn_h.shape = (batch, rnn_hidden_size * 2)\n",
        "        \"\"\"\n",
        "        x_embedded = self.source_embedding(x_source)\n",
        "        x_lengths = x_lengths.detach().cpu().numpy()\n",
        "\n",
        "        # create PackedSequence;\n",
        "        # x_packed.data.shape=(number_items, embedding_size)\n",
        "        x_packed = pack_padded_sequence(x_embedded, x_lengths, batch_first=True)\n",
        "        \n",
        "        # x_birnn_h.shape = (num_rnn, batch_size, feature_size)\n",
        "        x_birnn_out, x_birnn_h = self.birnn(x_packed)\n",
        "        # permute to (batch_size, num_rnn, feature_size)\n",
        "        x_birnn_h = x_birnn_h.permute(1,0,2)\n",
        "        \n",
        "        # flatten features; reshape to (batch_size, num_rnn * feature_size)\n",
        "        #  (recall: -1 takes the remaining positions, \n",
        "        #           flattening the two RNN hidden vectors into 1)\n",
        "        x_birnn_h = x_birnn_h.contiguous().view(x_birnn_h.size(0), -1)\n",
        "        \n",
        "        x_unpacked, _ = pad_packed_sequence(x_birnn_out, batch_first=True)\n",
        "        return x_unpacked, x_birnn_h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_Rvn02fpYqF"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92kP8qITpYqH"
      },
      "source": [
        "def verbose_attention(encoder_state_vectors, query_vector):\n",
        "    \"\"\"A descriptive version of the neural attention mechanism \n",
        "    \n",
        "    Args:\n",
        "        encoder_state_vectors (torch.Tensor): 3dim tensor from bi-GRU in encoder\n",
        "        query_vector (torch.Tensor): hidden state in decoder GRU\n",
        "    \"\"\"\n",
        "    batch_size, num_vectors, vector_size = encoder_state_vectors.size()\n",
        "    \n",
        "    vector_scores = \\\n",
        "        torch.sum(encoder_state_vectors * query_vector.view(batch_size, 1, vector_size), dim=2)\n",
        "    \n",
        "    vector_probabilities = torch.softmax(vector_scores, dim=1)\n",
        "    \n",
        "    weighted_vectors = \\\n",
        "        encoder_state_vectors * vector_probabilities.view(batch_size, num_vectors, 1)\n",
        "    \n",
        "    context_vectors = torch.sum(weighted_vectors, dim=1)\n",
        "    return context_vectors, vector_probabilities, vector_scores\n",
        "\n",
        "def terse_attention(encoder_state_vectors, query_vector):\n",
        "    \"\"\"A shorter and more optimized version of the neural attention mechanism\n",
        "    \n",
        "    Args:\n",
        "        encoder_state_vectors (torch.Tensor): 3dim tensor from bi-GRU in encoder\n",
        "        query_vector (torch.Tensor): hidden state\n",
        "    \"\"\"\n",
        "    vector_scores = torch.matmul(encoder_state_vectors, query_vector.unsqueeze(dim=2)).squeeze()\n",
        "    \n",
        "    vector_probabilities = torch.softmax(encoder_state_vectors, dim=-1)\n",
        "    context_vectors = torch.matmul(encoder_state_vectors.transpose(-2, -1),\n",
        "                                  vector_probabilities.unsqueeze(dim=2)).squeeze()\n",
        "    \n",
        "    return context_vectors, vector_probabilities\n",
        "\n",
        "class NMTDecoder(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_size, rnn_hidden_size, bos_index):\n",
        "        super(NMTDecoder, self).__init__()\n",
        "        \n",
        "        self._rnn_hidden_size = rnn_hidden_size\n",
        "        self.target_embedding = nn.Embedding(num_embeddings=num_embeddings,\n",
        "                                            embedding_dim=embedding_size,\n",
        "                                            padding_idx=0)\n",
        "        \n",
        "        self.gru_cell = nn.GRUCell(embedding_size + rnn_hidden_size, rnn_hidden_size)\n",
        "        self.hidden_map = nn.Linear(rnn_hidden_size, rnn_hidden_size)\n",
        "        \n",
        "        self.classifier = nn.Linear(rnn_hidden_size*2, num_embeddings)\n",
        "        self.bos_index = bos_index\n",
        "        \n",
        "    def _init_indices(self, batch_size):\n",
        "        \"\"\"returns the BOS index vector\"\"\"\n",
        "        return torch.ones(batch_size, dtype=torch.int64) * self.bos_index\n",
        "            \n",
        "    def _init_context_vectors(self, batch_size):\n",
        "        \"\"\"returns a zeros vector for initializing the context\"\"\"\n",
        "        return torch.zeros(batch_size, self._rnn_hidden_size)\n",
        "    \n",
        "    def forward(self, encoder_state, initial_hidden_state, target_sequence):\n",
        "        \"\"\"The forward pass of the model\n",
        "        \n",
        "        Args:\n",
        "            encoder_state (torch.Tensor): the output of the NMTEncoder\n",
        "            initial_hidden_state (torch.Tensor): The last hidden state in the  NMTEncoder\n",
        "            target_sequence (torch.Tensor): the target text data tensor\n",
        "        Returns:\n",
        "            output_vectors (torch.Tensor): prediction vectors at each output step\n",
        "        \"\"\"\n",
        "\n",
        "        target_sequence = target_sequence.permute(1,0)\n",
        "        \n",
        "        h_t = self.hidden_map(initial_hidden_state)\n",
        "        \n",
        "        batch_size = encoder_state.size(0)\n",
        "        \n",
        "        # initialize context vector\n",
        "        context_vectors = self._init_context_vectors(batch_size)\n",
        "        y_t_index = self._init_indices(batch_size)\n",
        "        \n",
        "        h_t = h_t.to(encoder_state.device)\n",
        "        y_t_index = y_t_index.to(encoder_state.device)\n",
        "        context_vectors = context_vectors.to(encoder_state.device)\n",
        "        \n",
        "        output_vectors = []\n",
        "        self._cached_p_attn = []\n",
        "        self._cached_ht = []\n",
        "        self._cached_decoder_state = encoder_state.cpu().detach().numpy()\n",
        "        \n",
        "        output_sequence_size = target_sequence.size(0)\n",
        "        for i in range(output_sequence_size):\n",
        "\n",
        "            y_t_index = target_sequence[i]\n",
        "            \n",
        "            # decoding the vectors\n",
        "            # 1. embed word and concat with previous context\n",
        "            y_input_vector = self.target_embedding(y_t_index)\n",
        "            rnn_input = torch.cat([y_input_vector, context_vectors], dim=1)\n",
        "            \n",
        "            # 2. make a GRU step, getting a new hidden vector\n",
        "            h_t = self.gru_cell(rnn_input, h_t)\n",
        "            self._cached_ht.append(h_t.cpu().detach().numpy())\n",
        "            \n",
        "            # 3. use current vector to attend to encoder state\n",
        "            context_vectors, p_attn, _ = verbose_attention(encoder_state_vectors=encoder_state,\n",
        "                                                           query_vector = h_t)\n",
        "            \n",
        "            # cache the attention probabilities for visualization\n",
        "            self._cached_p_attn.append(p_attn.cpu().detach().numpy())\n",
        "            \n",
        "            # 4 use current hidden and context vectors\n",
        "            # to make a prediction for the next word\n",
        "            prediction_vector = torch.cat((context_vectors, h_t), dim=1)\n",
        "            score_for_y_t_index = self.classifier(prediction_vector)\n",
        "            \n",
        "            # collect the prediction scores\n",
        "            output_vectors.append(score_for_y_t_index)\n",
        "\n",
        "        output_vectors = torch.stack(output_vectors).permute(1, 0, 2)\n",
        "        \n",
        "        return output_vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYUW_rnapYqN"
      },
      "source": [
        "## NMT Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73udPqBNpYqO"
      },
      "source": [
        "class NMTModel(nn.Module):\n",
        "    def __init__(self, source_vocab_size, source_embedding_size, target_vocab_size,\n",
        "                target_embedding_size, encoding_size, target_bos_index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            source_vocab_size (int): number of unique words in source language\n",
        "            source_embedding_size (int): size of the source embedding vectors\n",
        "            target_vocab_size (int): number of unique words in target language\n",
        "            target_embedding_size (int): size of the target embedding vectors\n",
        "            encoding_size (int): the size of the encoder RNN.  \n",
        "        \"\"\"\n",
        "        super(NMTModel, self).__init__()\n",
        "        \n",
        "        self.encoder = NMTEncoder(num_embeddings= source_vocab_size,\n",
        "                                 embedding_size=source_embedding_size,\n",
        "                                 rnn_hidden_size=encoding_size)\n",
        "        \n",
        "        decoding_size = encoding_size * 2\n",
        "        \n",
        "        self.decoder = NMTDecoder(num_embeddings= target_vocab_size,\n",
        "                                 embedding_size= target_embedding_size,\n",
        "                                 rnn_hidden_size= decoding_size,\n",
        "                                 bos_index= target_bos_index)\n",
        "        \n",
        "    def forward(self, x_source, x_source_lengths, target_sequence):\n",
        "        \"\"\"\n",
        "        The forward pass of the model\n",
        "        \n",
        "        Args:\n",
        "            x_source (torch.Tensor): the source text data tensor. \n",
        "                x_source.shape should be (batch, vectorizer.max_source_length)\n",
        "            x_source_lengths torch.Tensor): the length of the sequences in x_source \n",
        "            target_sequence (torch.Tensor): the target text data tensor\n",
        "        Returns:\n",
        "            decoded_states (torch.Tensor): prediction vectors at each output step\n",
        "        \"\"\"\n",
        "        encoder_state, final_hidden_states = self.encoder(x_source,\n",
        "                                                         x_source_lengths)\n",
        "        decoded_states = self.decoder(encoder_state, final_hidden_states,\n",
        "                                     target_sequence)\n",
        "        \n",
        "        return decoded_states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8f7XFJppYqV"
      },
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTCRhwTJpYqX"
      },
      "source": [
        "def compute_accuracy(y_pred, y_true, mask_index):\n",
        "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
        "\n",
        "    _, y_pred_indices = y_pred.max(dim=1)\n",
        "    \n",
        "    correct_indices = torch.eq(y_pred_indices, y_true).float()\n",
        "    valid_indices = torch.ne(y_true, mask_index).float()\n",
        "    \n",
        "    n_correct = (correct_indices * valid_indices).sum().item()\n",
        "    n_valid = valid_indices.sum().item()\n",
        "\n",
        "    return n_correct / n_valid * 100\n",
        "\n",
        "def normalize_sizes(y_pred, y_true):\n",
        "    \"\"\"Normalize tensor sizes\n",
        "    \n",
        "    Args:\n",
        "        y_pred (torch.Tensor): the output of the model\n",
        "            If a 3-dimensional tensor, reshapes to a matrix\n",
        "        y_true (torch.Tensor): the target predictions\n",
        "            If a matrix, reshapes to be a vector\n",
        "    \"\"\"\n",
        "    if len(y_pred.size()) == 3:\n",
        "        y_pred = y_pred.contiguous().view(-1, y_pred.size(2))\n",
        "    if len(y_true.size()) == 2:\n",
        "        y_true = y_true.contiguous().view(-1)\n",
        "    return y_pred, y_true\n",
        "\n",
        "def sequence_loss(y_pred, y_true, mask_index):\n",
        "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
        "    return F.cross_entropy(y_pred, y_true, ignore_index=mask_index)\n",
        "\n",
        "\n",
        "def update_train_state(args, model, train_state):\n",
        "    \"\"\"\n",
        "    Update model and early stopping\n",
        "    \"\"\"\n",
        "    if train_state['epoch_index'] == 0:\n",
        "        torch.save(model.state_dict(), train_state['model_filename'])\n",
        "\n",
        "    # save model if performance improved\n",
        "    elif train_state['epoch_index'] >= 1:\n",
        "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
        "         \n",
        "        # If loss worsened\n",
        "        if loss_t >= loss_tm1:\n",
        "            # Update step\n",
        "            train_state['early_stopping_step'] += 1\n",
        "        # Loss decreased\n",
        "        else:\n",
        "            # Save the best model\n",
        "            if loss_t < train_state['early_stopping_best_val']:\n",
        "                torch.save(model.state_dict(), train_state['model_filename'])\n",
        "                train_state['early_stopping_best_val'] = loss_t\n",
        "\n",
        "            # Reset early stopping step\n",
        "            train_state['early_stopping_step'] = 0\n",
        "\n",
        "        # Stop early ?\n",
        "        train_state['stop_early'] = \\\n",
        "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
        "\n",
        "    return train_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1N_IWxtopYqg"
      },
      "source": [
        "## Training Routine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_0rMHJ_pYqh"
      },
      "source": [
        "args = Namespace(\n",
        "    # Data information\n",
        "    frequency_cutoff = 25,\n",
        "    text_csv = '/content/drive/My Drive/Colab Notebooks/PoetryGenerator/data/poet_dataframe_sentences.csv',\n",
        "    model_filename = '/content/drive/My Drive/Colab Notebooks/PoetryGenerator/model/poet_GRU_model_state.pth',\n",
        "    # Model HyperParameters\n",
        "    source_embedding_size=100,\n",
        "    target_embedding_size=100,\n",
        "    encoding_size=64,\n",
        "    # Training HyperParameters\n",
        "    batch_size = 128,\n",
        "    early_stopping_criteria=5,\n",
        "    learning_rate=0.001,\n",
        "    momentum=0.1,\n",
        "    num_epochs=100,\n",
        "    seed=1337,\n",
        "    cuda=True,\n",
        "    dropout=0.1\n",
        ")\n",
        "\n",
        "def make_train_state(args):\n",
        "    return {'stop_early': False,\n",
        "            'early_stopping_step': 0,\n",
        "            'early_stopping_best_val': 1e8,\n",
        "            'learning_rate': args.learning_rate,\n",
        "            'epoch_index': 0,\n",
        "            'train_loss': [],\n",
        "            'train_acc': [],\n",
        "            'val_loss': [],\n",
        "            'val_acc': [],\n",
        "            'test_loss': -1,\n",
        "            'test_acc': -1,\n",
        "            'model_filename': args.model_filename\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBnz76VFpYqq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0b1216b-0fa0-4030-c4bd-ff2f19e49589"
      },
      "source": [
        "train_state = make_train_state(args)\n",
        "\n",
        "if torch.cuda.is_available() and args.cuda:\n",
        "  args.cuda = True\n",
        "else:\n",
        "  args.cuda = False\n",
        "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "print(\"Device available \", args.device)\n",
        "\n",
        "# dataset object\n",
        "dataset = NMTDataset.load_dataset_and_make_vectorizer(args.text_csv)\n",
        "\n",
        "# vectorizer\n",
        "vectorizer = dataset.get_vectorizer()\n",
        "\n",
        "# classifier\n",
        "model = NMTModel(source_vocab_size= len(vectorizer.source_vocab),\n",
        "                target_vocab_size= len(vectorizer.target_vocab),\n",
        "                source_embedding_size = args.source_embedding_size,\n",
        "                target_embedding_size= args.target_embedding_size,\n",
        "                encoding_size= args.encoding_size,\n",
        "                target_bos_index= vectorizer.target_vocab.begin_seq_index)\n",
        "model.to(args.device)\n",
        "\n",
        "# optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device available  cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxi5MEUqpYqy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "7c315517838442eea5cd262322d45278",
            "e09f7fbaf8b640c795a399f11e351d25",
            "f2bb7ec41a8c47d69ec6c540864a36af",
            "a0c556df208c4dfc90b8597e8cbec2fa",
            "67b9a5cd58c8455a89c9a981c4aee042",
            "99bd9c81451b41699dd6ca623e82829e",
            "ddec5362159d40da850556e39c441af2",
            "0b5ffecce8f2478394f8639191a659f4",
            "4d665657121d4044ae287f1fda164d51",
            "fd21a161ba354202a2097b1690ec2867",
            "d9d365346f1645fbb10ef18779b82dd0",
            "14780bfe300841d6882ace9e12a3c725",
            "9538779411644b4da1774a4557b92d90",
            "f2b0c6f19d754fb5970ac621785c54db",
            "c70069eb7fbc4061b8e5570ca176a163",
            "158f92552a43458c9b9ae46170c47eda",
            "45f267023c55457a9e08dc54d4e5721b",
            "55d09f1310e34394b7c76c77074b7c91",
            "eee5a78fcafc448da9eaf3456c1e9648",
            "138a29d029b947aebdfb1e3510fe51d8",
            "146bcee8e92544048999e086cb62dfad",
            "22f2f3ffad6949959011617388ebc0eb",
            "5be9a88ded444eef8bf4e5528e092881",
            "7f564389953148fc95b9707870a35ebf"
          ]
        },
        "outputId": "4a65052e-749c-48c9-a0e7-dde86d0a2e2d"
      },
      "source": [
        "mask_index = vectorizer.target_vocab.mask_index\n",
        "epoch_bar = tqdm(desc='training routine', \n",
        "                          total=args.num_epochs,\n",
        "                          position=0)\n",
        "\n",
        "dataset.set_split('train')\n",
        "train_bar = tqdm(desc='split=train',\n",
        "                          total=dataset.get_num_batches(args.batch_size), \n",
        "                          position=0, \n",
        "                          leave=True)\n",
        "dataset.set_split('val')\n",
        "val_bar = tqdm(desc='split=val',\n",
        "                        total=dataset.get_num_batches(args.batch_size), \n",
        "                        position=0, \n",
        "                        leave=True)\n",
        "\n",
        "try:\n",
        "    for epoch_index in range(args.num_epochs):\n",
        "        train_state['epoch_index'] = epoch_index\n",
        "\n",
        "        # Iterate over training dataset\n",
        "\n",
        "        # setup: batch generator, set loss and acc to 0, set train mode on\n",
        "        dataset.set_split('train')\n",
        "        batch_generator = generate_nmt_batches(dataset, \n",
        "                                           batch_size=args.batch_size, \n",
        "                                           device=args.device)\n",
        "        running_loss = 0.0\n",
        "        running_acc = 0.0\n",
        "        model.train()\n",
        "        \n",
        "        for batch_index, batch_dict in enumerate(batch_generator):\n",
        "            # the training routine is these 5 steps:\n",
        "\n",
        "            # --------------------------------------    \n",
        "            # step 1. zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # step 2. compute the output\n",
        "            y_pred = model(batch_dict['x_source'], \n",
        "                           batch_dict['x_source_length'], \n",
        "                           batch_dict['x_target'])\n",
        "\n",
        "            # step 3. compute the loss\n",
        "            loss = sequence_loss(y_pred, batch_dict['y_target'], mask_index)\n",
        "\n",
        "\n",
        "            # step 4. use loss to produce gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # step 5. use optimizer to take gradient step\n",
        "            optimizer.step()\n",
        "            # -----------------------------------------\n",
        "            # compute the  running loss and running accuracy\n",
        "            running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
        "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n",
        "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "            # update bar\n",
        "            train_bar.set_postfix(loss=running_loss,\n",
        "                                  acc=running_acc,\n",
        "                                  epoch=epoch_index)\n",
        "            train_bar.update()\n",
        "\n",
        "        train_state['train_loss'].append(running_loss)\n",
        "        train_state['train_acc'].append(running_acc)\n",
        "\n",
        "        # Iterate over val dataset\n",
        "\n",
        "        # setup: batch generator, set loss and acc to 0; set eval mode on\n",
        "        dataset.set_split('val')\n",
        "        batch_generator = generate_nmt_batches(dataset, \n",
        "                                           batch_size=args.batch_size, \n",
        "                                           device=args.device)\n",
        "        running_loss = 0.\n",
        "        running_acc = 0.\n",
        "        model.eval()\n",
        "\n",
        "        for batch_index, batch_dict in enumerate(batch_generator):\n",
        "            # compute the output\n",
        "            y_pred = model(batch_dict['x_source'], \n",
        "                           batch_dict['x_source_length'], \n",
        "                           batch_dict['x_target'])\n",
        "\n",
        "            # step 3. compute the loss\n",
        "            loss = sequence_loss(y_pred, batch_dict['y_target'], mask_index)\n",
        "\n",
        "            # compute the  running loss and running accuracy\n",
        "            running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
        "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n",
        "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "            \n",
        "            # Update bar\n",
        "            val_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
        "                            epoch=epoch_index)\n",
        "            val_bar.update()\n",
        "\n",
        "        train_state['val_loss'].append(running_loss)\n",
        "        train_state['val_acc'].append(running_acc)\n",
        "\n",
        "        train_state = update_train_state(args=args, model=model, \n",
        "                                         train_state=train_state)\n",
        "        \n",
        "        train_bar.n = 0\n",
        "        val_bar.n = 0\n",
        "        epoch_bar.update()\n",
        "        \n",
        "except KeyboardInterrupt:\n",
        "    print(\"Exiting loop\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c315517838442eea5cd262322d45278",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='training routine', style=ProgressStyle(description_width=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4d665657121d4044ae287f1fda164d51",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='split=train', max=14.0, style=ProgressStyle(description_w…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "45f267023c55457a9e08dc54d4e5721b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='split=val', max=3.0, style=ProgressStyle(description_widt…"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVqhYF4iITXa"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2h9CbwuHgrV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4ac9ae8-415e-4e1d-bcce-a6b5c42d5055"
      },
      "source": [
        "def get_source_sentence(vectorizer, batch_dict, index):\n",
        "    indices = batch_dict['x_source'][index].cpu().data.numpy()\n",
        "    vocab = vectorizer.source_vocab\n",
        "    return sentence_from_indices(indices, vocab)\n",
        "\n",
        "def get_true_sentence(vectorizer, batch_dict, index):\n",
        "    return sentence_from_indices(batch_dict['y_target'].cpu().data.numpy()[index], vectorizer.target_vocab)\n",
        "    \n",
        "def get_sampled_sentence(vectorizer, batch_dict, index):\n",
        "    y_pred = model(x_source=batch_dict['x_source'], \n",
        "                   x_source_lengths=batch_dict['x_source_length'], \n",
        "                   target_sequence=batch_dict['x_target'])\n",
        "    return sentence_from_indices(torch.max(y_pred, dim=2)[1].cpu().data.numpy()[index], vectorizer.target_vocab)\n",
        "\n",
        "def get_all_sentences(vectorizer, batch_dict, index):\n",
        "    return {\"source\": get_source_sentence(vectorizer, batch_dict, index), \n",
        "            \"truth\": get_true_sentence(vectorizer, batch_dict, index), \n",
        "            \"sampled\": get_sampled_sentence(vectorizer, batch_dict, index)}\n",
        "    \n",
        "def sentence_from_indices(indices, vocab, strict=True):\n",
        "    ignore_indices = set([vocab.mask_index, vocab.begin_seq_index, vocab.end_seq_index])\n",
        "    out = []\n",
        "    for index in indices:\n",
        "        if index == vocab.begin_seq_index and strict:\n",
        "            continue\n",
        "        elif index == vocab.end_seq_index and strict:\n",
        "            return \" \".join(out)\n",
        "        else:\n",
        "            out.append(vocab.lookup_index(index))\n",
        "    return \" \".join(out)\n",
        "\n",
        "\n",
        "dataset.set_split('test')\n",
        "batch_generator = generate_nmt_batches(dataset, \n",
        "                                       batch_size=args.batch_size, \n",
        "                                       device=args.device)\n",
        "\n",
        "test_sample = '/content/drive/MyDrive/Colab Notebooks/PoetryGenerator/data/test_sample.txt'\n",
        "val_sample = '/content/drive/MyDrive/Colab Notebooks/PoetryGenerator/data/val_sample.txt'\n",
        "\n",
        "for batch_dict in batch_generator:\n",
        "    results = get_all_sentences(vectorizer, batch_dict, 0)\n",
        "    print('-'*100)\n",
        "    print(\"SOURCE : \", results['source'])\n",
        "    print(\"SAMPLED: \", results['sampled'])\n",
        "    print(\"TRUTH: \",results['truth'])\n",
        "    \n",
        "    with open(test_sample, 'a') as fp:\n",
        "        fp.write(\"-\"*100)\n",
        "        fp.write(\"\\n\")\n",
        "        fp.write(\"SOURCE : {}\".format(results['source']))\n",
        "        fp.write(\"\\n\")\n",
        "        fp.write(\"SAMPLED : {}\".format(results['sampled'])) \n",
        "        fp.write(\"\\n\")\n",
        "        fp.write(\"TRUTH : {}\".format(results['truth']))\n",
        "        fp.write(\"\\n\")\n",
        "\n",
        "    fp.close()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "SOURCE :  i the bronze <UNK> grant riding a bronze horse in <UNK> <UNK> <UNK> in the sun by day when the motor <UNK> <UNK> by in long <UNK> <UNK> somewhere to keep <UNK> for <UNK> and <UNK> and <UNK> and <UNK> though in the dusk and <UNK> when high waves are <UNK> on the <UNK> of the <UNK> along the lake shore near by i have seen the <UNK> dare the <UNK> come <UNK> and make to ride his bronze horse out into the hoofs and <UNK> of the storm . \n",
            "SAMPLED:  and i did it the the the wind , . it wind wind simple out \n",
            "TRUTH:  ii i cross <UNK> <UNK> on a winter night when the snow is falling . \n",
            "----------------------------------------------------------------------------------------------------\n",
            "SOURCE :  thereto do thou , great goddess , queen of beauty , mother of love , and of all world s delight , without whose sovereign grace and kindly duty nothing on earth seems fair to fleshly sight , do thou <UNK> with thy love <UNK> light t <UNK> my dim and <UNK> <UNK> , and beautify this sacred hymn of thine that both to thee , to whom i mean it most , and eke to her , whose fair immortal beam hath darted fire into my feeble ghost , that now it wasted is with woes extreme , it may so please , that she at length will stream some dew of grace into my withered heart , after long sorrow and <UNK> smart . \n",
            "SAMPLED:  she she it thy s loyal face , , and be her , pure an she are . from and is so path is us dame . us the with thousand light that from th head heart with sorrow . by with the . the could say her her goodly , from her may and that she now be shown , with \n",
            "TRUTH:  what time this world s great <UNK> did cast to make all things such as we now behold , it seems that he before his eyes had <UNK> d a goodly pattern , to whose perfect <UNK> he fashion d them as comely as he could that now so fair and <UNK> they appear , as nought may be amended <UNK> . \n",
            "----------------------------------------------------------------------------------------------------\n",
            "SOURCE :  calm was the day , and through the trembling air sweet breathing zephyrus did softly play , a gentle spirit , that lightly did delay hot titan s beams , which then did glister fair when i whose sullen care , through discontent of my long fruitless stay in prince s court , and expectation vain of idle hopes , which still do fly away like empty shadows , did afflict my brain , walked forth to ease my pain along the shore of silver streaming thames , whose rutty bank , the which his river hems , was painted all with variable flowers , and all the meads adorned with dainty gems , fit to deck maidens bowers , and crown their paramours , against the bridal day , which is not long sweet thames , run softly , till i end my song . \n",
            "SAMPLED:  there , in a meadow , that flowers river s side , and flock of nymphs , chanced to espy , all loose daughters of the flood thereby , and goodly greenish locks , all loose untied , and all that been a bride and make one that a little wicker basket , and of thy twigs , entrailed curiously , and which they may flowers to quake , flasket , and with fine fingers cropt full featously the tender stalks on high . \n",
            "TRUTH:  there , in a meadow , by the river s side , a flock of nymphs i chanced to espy , all lovely daughters of the flood thereby , with goodly greenish locks , all loose untied , as each had been a bride and each one had a little wicker basket , made of fine twigs , entrailed curiously , in which they gathered flowers to fill their flasket , and with fine fingers cropt full featously the tender stalks on high . \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}